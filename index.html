<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="keywords" content="Zhikang Niu, ÁâõÂøóÂ∫∑, Audio Signal Processing, Speech Multimodal Large Language Model, Deep Learning, machine learning, Xidian University">
<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Spectral:wght@400;500;600&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./images/book.png">
<title>Zhikang Niu (ÁâõÂøóÂ∫∑)</title>

<!-- ============ Add styles for Education & Intern alignment ============ -->
<style type="text/css">
  /* ‰∏§ÂàóÂØπÈΩêÔºöÂ∑¶ÂàóÊó•ÊúüÔºåÂè≥ÂàóËØ¶ÊÉÖ */
  .edu-list { list-style: none; padding: 0; margin: 0; }
  .edu-list li {
    display: grid;
    grid-template-columns: 170px auto; /* Ë∞ÉÊï¥Â∑¶ÂàóÂÆΩÂ∫¶Âç≥ÂèØÂæÆË∞ÉÂØπÈΩê */
    column-gap: 12px;
    margin-bottom: 8px;
    align-items: start;
  }
  .edu-list .date { font-weight: bold; white-space: nowrap; }
  .edu-list .detail { line-height: 1.5; }
  .edu-list a { word-break: break-word; }
  .edu-note { color: #5c657e; font-size: 0.95em; }
  @media (max-width: 760px) {
    .edu-list li { grid-template-columns: 1fr; row-gap: 4px; }
    .edu-list .date { font-weight: 600; }
  }
</style>
<!-- ==================================================================== -->

</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<div class="hero">
<table class="imgtable"><tr><td>
<a href="./"><img src="./images/zhikangniu_cartoon.png" alt="" height="220" class="profile-photo" /></a>&nbsp;</td>
<td align="left"><h1><a href="./">Zhikang Niu (<span style="font-family:KaiTi, STKaiti, KaiTi_GB2312, serif">ÁâõÂøóÂ∫∑</span>)</a></h1>
<!-- <br /> -->
Ph.D. Student. 
<a href="https://x-lance.github.io/" target="_blank">Cross Media (X-)Language Intelligence Lab</a><br />
<a href="https://www.cs.sjtu.edu.cn/" target="_blank">Department of Computer Science and Engineering</a>, <a href="https://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University.</a><br />
Work at Shanghai Innovation Institute.
<br />
<!-- <br /> -->
<div class="contact-meta">
  <span class="meta-item">
    <span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M12 2c3.86 0 7 3.13 7 7 0 5.25-7 13-7 13S5 14.25 5 9c0-3.87 3.14-7 7-7zm0 3.3a3.7 3.7 0 1 0 0 7.4 3.7 3.7 0 0 0 0-7.4z" /></svg></span>
    Shanghai, China
  </span>
  <span class="meta-divider" aria-hidden="true">‚Ä¢</span>
  <span class="meta-item">
    <span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M12 2a10 10 0 1 0 .01 20.01A10 10 0 0 0 12 2zm0 1.8a8.2 8.2 0 1 1 0 16.4 8.2 8.2 0 0 1 0-16.4zm-.9 3.3v5.2l4.6 2.7.9-1.5-3.7-2.1V7.1h-1.8z" /></svg></span>
    <span id="shanghai-time" aria-live="polite"></span>
  </span>
</div>
<div class="staffshortcut">
 <a href="#Interest">Research Interests</a>
 <a href="#Education">Education</a>
 <a href="#Research Experience">Research Experience</a>
 <a href="#Publications">Publications</a>
 <a href="#Research Projects">Research Projects</a>
 <a href="#Honors and awards">Honors and Awards</a>
 <a href="#Activities">Activities</a>
 <!-- <A HREF="#Invited talks and lives">Invited Talks and Lives</A> -->
</div>
<div class="contact-note">
if you have any questions, please feel free to contact me with <a href="mailto:zhikangniu@sjtu.edu.cn"><span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M3 5h18a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V7a2 2 0 0 1 2-2zm0 2v.01l9 6 9-6V7H3zm18 10V9l-8.4 5.6a1 1 0 0 1-1.2 0L3 9v8h18z" /></svg></span>zhikangniu@sjtu.edu.cn</a>
</div>
<!-- Ê∑ªÂä†ÈÇÆ‰ª∂ -->
<!-- [<a href="mailto:zhikangniu@sjtu.edu.cn">Email</a>] -->
<div class="contact-links">
[<a href="https://github.com/ZhikangNiu" target="_blank"><span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" focusable="false"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z" /></svg></span>GitHub</a>] 
[<a href="https://scholar.google.com/citations?user=mXSpi2kAAAAJ&hl=en" target="_blank"><span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M12 3 1 8l11 5 9-4.09V15h2V8L12 3zm-7 9.5V17c0 .38.22.72.56.88L12 20l6.44-2.12A1 1 0 0 0 19 17v-4.5l-7 3.5-7-3.5z" /></svg></span>Google Scholar</a>] 
[<a href="./assets/WeChat_QR.jpg" target="_blank"><span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M9.5 3C5.36 3 2 5.69 2 9c0 2.14 1.41 4.03 3.5 5.08L5 18l3.33-2.11c.38.05.76.07 1.17.07.08 0 .16 0 .24-.01A6.17 6.17 0 0 1 9 13c0-3.17 2.69-5.75 6-5.75.24 0 .48.01.71.04C14.79 4.77 12.36 3 9.5 3zm-3 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2zm5 0a1 1 0 1 1 0-2 1 1 0 0 1 0 2z" /><path d="M15 8c3.31 0 6 2.46 6 5.5 0 1.83-1.05 3.45-2.67 4.47L18 21l-2.88-1.72c-.36.05-.74.07-1.12.07-3.31 0-6-2.46-6-5.5S10.69 8 14 8h1zm-3 5a1 1 0 1 0 0 2 1 1 0 0 0 0-2zm5 0a1 1 0 1 0 0 2 1 1 0 0 0 0-2z" /></svg></span>WeChat</a>]
<!-- [<a href="./assets/resume-zh_CN.pdf" target="_blank">Chinese Resume</a>] -->
[<a href="./assets/resume_en.pdf" target="_blank"><span class="icon" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M6 2h8l6 6v12a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2zm8 1.5V9h5.5L14 3.5z" /><path d="M8 12h8v1.6H8V12zm0 3.5h8v1.6H8v-1.6z" /></svg></span>Resume</a>]
</div>
<script type="text/javascript">
  (function () {
    var timeEl = document.getElementById("shanghai-time");
    if (!timeEl || !window.Intl) {
      return;
    }
    var formatter = new Intl.DateTimeFormat("en-GB", {
      timeZone: "Asia/Shanghai",
      hour: "2-digit",
      minute: "2-digit",
      hour12: false
    });
    function updateTime() {
      timeEl.textContent = formatter.format(new Date());
    }
    updateTime();
    setInterval(updateTime, 30000);
  })();
</script>
</td></tr></table>
</div>


 
<a name="Interest"><h2>Research Interest</h2></a>
I work in the field of Audio Singal Processing, Audio Codec Model,Multimodal Large Language Model, Machine learning, and Deep learning supervised by <a href="https://chenxie95.github.io/" target="_blank">Prof. Xie Chen</a>, I will try my best in the next five exciting years! üí™. Currently, I focus on the following research topics:
<ul>
<li>Audio Tokenizer (Discrete and Continuous)</li>
<li>Speech Synthesis (Text to Speech)</li>
<li>Multimodal Large Language Model</li>
</ul>


<!-- ===================== Education (aligned) ===================== -->
<a name="Education"><h2>Education</h2></a>
<ul class="edu-list">
  <li>
    <span class="date">2024.09‚Äì2029.06</span>
    <span class="detail">Ph.D. Student, <a href="https://www.cs.sjtu.edu.cn/" target="_blank">Department of Computer Science and Engineering</a>, <a href="https://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a>. </span>
  </li>
  <li>
    <span class="date">2020.09‚Äì2024.06</span>
    <span class="detail">Bachelor Degree of Engineering, <a href="https://sai.xidian.edu.cn/" target="_blank">School of Artificial Intelligence</a>, <a href="https://www.xidian.edu.cn/" target="_blank">Xidian University</a>.</span>
  </li>
</ul>
<!-- ================================================================= -->

<!-- ===================== Internships (aligned) ===================== -->
<a name="Research Experience"><h2>Research Experience</h2></a>
<ul class="edu-list">
  <li>
    <span class="date">2025.07‚Äìcurrent</span>
    <span class="detail">Research Intern, <a href="https://www.minimaxi.com/" target="_blank">Minimax Speech Team</a>.</span>
  </li>
  <li>
    <span class="date">2025.01‚Äì2025.06</span>
    <span class="detail">Research Intern, <a href="https://www.shlab.org.cn/" target="_blank">Shanghai Artificial Intelligence Laboratory</a>.</span>
  </li>
  <li>
    <span class="date">2023.08‚Äì2024.09</span>
    <span class="detail">Research Intern, Natural Language Computing Group (NLC), <a href="https://www.msra.cn/" target="_blank">Microsoft Research Asia (MSRA)</a>. Led by <a href="https://thegenerality.com/" target="_blank">Furu Wei</a>, supervised by <a href="https://www.microsoft.com/en-us/research/people/shujliu/" target="_blank">Shujie Liu</a> and <a href="https://long-zhou.github.io/" target="_blank">Long Zhou</a>. Focus on Audio Codec and Speech Synthesis.</span>
  </li>
</ul>
<!-- =================================================================== -->

 
<a name="Publications"><h2>Publications</h2></a>
<a name="tts/omni"><h3>Speech Synthesis/Omni System</h3></a>
<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> <b>Zhikang Niu</b>, Sanyuan Chen, Long Zhou, Ziyang Ma, Xie Chen, Shujie Liu.<br />
<b>NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization.</b><br />
<i><b>SLT 2024</b></i>,
[<a href="https://arxiv.org/abs/2409.12717" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2409.12717.pdf" target="_blank">PDF</a>]
[<a href="https://github.com/ZhikangNiu/NDVQ" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:iZGY-fGY7V0J:scholar.google.com/&output=citation&scisdr=ClE9Ykj8EOKBzLPFleU:AFWwaeYAAAAAZwfDjeWrAPZjPpJAhx24s6n-28I&scisig=AFWwaeYAAAAAZwfDjRXdVka9BJmZmd1l7f2Pq5s&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
  <ul>
  <!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
  <li> <b>Zhikang Niu</b>, Shujie Hu, Jeongsoo Choi, Yushen Chen, Peining Chen, Pengcheng Zhu, Yunting Yang, Bowen Zhang, Jian Zhao, Chunhui Wang, Xie Chen.<br />
  <b>Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis.</b><br />
  <!-- <i><b>SLT 2024</b></i>, -->
  [<a href="https://arxiv.org/abs/2509.22167" target="_blank">Link</a>]
  [<a href="https://arxiv.org/pdf/2509.22167.pdf" target="_blank">PDF</a>]
  [<a href="https://github.com/ZhikangNiu/Semantic-VAE" target="_blank">Code</a>]
  [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:XzqJv8VaDKcJ:scholar.google.com/&output=citation&scisdr=CgKXACcqEOuD0V-L0t0:AAZF9b8AAAAAaOSNyt0sj-9fUPRFzZzm_Q1AMIs&scisig=AAZF9b8AAAAAaOSNykB5mxD-g8dGqtIAAo3tlg0&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
  </p>
  </ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Jeongsoo Choi<sup>*</sup>, <b>Zhikang Niu<sup>*</sup></b>, Ji-Hoon Kim, Chunhui Wang, Joon Son Chung, Xie Chen.<br />
<b>Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment.</b><br />
<i><b>InterSpeech 2025 Oral</b></i>,
[<a href="https://arxiv.org/abs/2505.19595" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2505.19595" target="_blank">PDF</a>]
[<a href="https://github.com/ZhikangNiu/A-DMA" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:exsTSNIez1gJ:scholar.google.com/&output=citation&scisdr=CgLF-yroEJq-_GfWPvo:AAZF9b8AAAAAaKXQJvozfAe9NZnbQnkuYcPdnfU&scisig=AAZF9b8AAAAAaKXQJhKBS4kaD48GEBTGEONKZoE&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li> Yushen Chen, <b>Zhikang Niu</b>, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen.<br />
<b>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.</b><br />
<i><b>ACL 2025 Main</b></i>,
[<a href="https://arxiv.org/abs/2410.06885" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2410.06885" target="_blank">PDF</a>]
[<a href="https://github.com/SWivid/F5-TTS" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:mabuw4cUBgYJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2Pm9TqM8I:AFWwaeYAAAAAZ2zsK8LxbCxd3NGHmZYe6ohDCeA&scisig=AFWwaeYAAAAAZ2zsK5y3MUlvYcPKpHnyK-5x0Oo&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
[<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>]<br />
<a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">13,000+ stars</font></b> on GitHub.<br />
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Wenhao Guan, <b>Zhikang Niu</b>, Ziyue Jiang, Kaidi Wang, Peijie Chen, Qingyang Hong, Lin Li, Xie Chen.<br />
<b>UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models.</b><br />
<!-- <i><b>InterSpeech 2025 Oral</b></i>, -->
[<a href="https://arxiv.org/abs/2510.04593" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2510.04593" target="_blank">PDF</a>]
[<a href="https://github.com/gwh22/UniVoice" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5jHD5dB85cwJ:scholar.google.com/&output=citation&scisdr=ChXMweiAEOuD0KTLpr8:ABGrvjIAAAAAaR_Nvr9Mqb4MXUYX0ZDVMJP97Fk&scisig=ABGrvjIAAAAAaR_Nvhmjy8_lVSNSlZEsE-rVa-A&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Qixi zheng, Yushen Chen, <b>Zhikang Niu</b>, Ziyang Ma, Xiaofei Wang, Kai Yu, Xie Chen.<br />
<b>Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling.</b><br />
<i><b>InterSpeech 2025</b></i>,
[<a href="https://arxiv.org/abs/2505.19931v1" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2505.19931v1.pdf" target="_blank">PDF</a>]
[<a href= "https://github.com/SWivid/F5-TTS" target="_blank">Code</a>]
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:iZGY-fGY7V0J:scholar.google.com/&output=citation&scisdr=ClE9Ykj8EOKBzLPFleU:AFWwaeYAAAAAZwfDjeWrAPZjPpJAhx24s6n-28I&scisig=AFWwaeYAAAAAZwfDjRXdVka9BJmZmd1l7f2Pq5s&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li>Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, <b>Zhikang Niu</b>, et al.<br />
<b>SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training.</b><br />
<i><b>ACL 2025 Findings</b></i>,
[<a href="https://arxiv.org/abs/2412.15649" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2412.15649" target="_blank">PDF</a>]
[<a href="https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/s2s" target="_blank">Code</a>]
[<a href="http://scholar.googleusercontent.com/scholar.bib?q=info:0ba6dZka5owJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2PlKmz4X0:AFWwaeYAAAAAaBG1-X04qYsd1dBVPEvMkKcpPb8&scisig=AFWwaeYAAAAAaBG1-R91WNJpVLpV4tYnSKLeoeA&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, <b>Zhikang Niu</b>, Shuai Wang, Hui Zhang, Xie Chen.<br />
<b>VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech.</b><br />
<i><b>ICASSP 2025</b></i>,
[<a href="https://arxiv.org/abs/2401.14321" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2401.14321.pdf" target="_blank">PDF</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:423LVFzDAh0J:scholar.google.com/&output=citation&scisdr=ClEXtQ9QEMjm1kg_lBQ:AFWwaeYAAAAAZeE5jBRZItSkmZL-6cy9PIFICTI&scisig=AFWwaeYAAAAAZeE5jKmKyS0hsW4Yk5zbPIk00-M&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Guanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, <b>Zhikang Niu</b>, Xie Chen.<br />
<b>Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.</b><br />
<i><b>ASRU 2023</b></i>,
[<a href="https://arxiv.org/abs/2309.13860" target="_blank">Link</a>]
[<a href="https://browse.arxiv.org/pdf/2309.13860.pdf" target="_blank">PDF</a>]
[<a href="https://github.com/yanghaha0908/FastHuBERT" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5igU4Z9Xe2sJ:scholar.google.com/&output=citation&scisdr=ClEoLDBxEOzd6Ydut8w:AFWwaeYAAAAAZSFor8xL1SdSCeYwkCTVnwkGQSo&scisig=AFWwaeYAAAAAZSFor1SCJ3LMXRhW0icv0dF0A4c&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
  <ul>
  <li> Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, <b>Zhikang Niu</b>, Yushen Chen, Ziyang Ma, et al.<br />
  <b>Towards Flow-Matching-based TTS without Classifier-Free Guidance.</b><br />
  <!-- <a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">11,000+ stars</font></b> on GitHub.<br> -->
  [<a href="https://arxiv.org/abs/2504.20334" target="_blank">Link</a>]
  [<a href="https://arxiv.org/pdf/2504.20334" target="_blank">PDF</a>]
  <!-- [<a href= "https://github.com/SWivid/F5-TTS" target="_blank">Code</a>] -->
  <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:mabuw4cUBgYJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2Pm9TqM8I:AFWwaeYAAAAAZ2zsK8LxbCxd3NGHmZYe6ohDCeA&scisig=AFWwaeYAAAAAZ2zsK5y3MUlvYcPKpHnyK-5x0Oo&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
  <!-- [<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>] -->
  </p>
  </ul>
  </font>
  
  <font size="3"> 
    <ul>
    <li> Xiquan Li, Junxi Liu, Yuzhe Liang, <b>Zhikang Niu</b>, Wenxi Chen, Xie Chen.<br />
    <b>MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows.</b>
    <!-- <a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">11,000+ stars</font></b> on GitHub.<br> -->
    <br>
    [<a href="https://arxiv.org/abs/2508.06098" target="_blank">Link</a>]
    [<a href="https://arxiv.org/pdf/2508.06098" target="_blank">PDF</a>]
    [<a href= "https://github.com/xiquan-li/MeanAudio" target="_blank">Code</a>]
    [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:z5x51JA1HcAJ:scholar.google.com/&output=citation&scisdr=CgKXACcqEOuD0V-JMCo:AAZF9b8AAAAAaOSPKCpl8M1RYE6AGAL8bbkorzY&scisig=AAZF9b8AAAAAaOSPKIB9vCenoguBZ4x24gwdQxY&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
    <!-- [<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>] -->
    </p>
    </ul>
  </font>
  
  <font size="3"> 
    <ul>
    <li>Yuxiang Zhao, Yunchong Xiao, Yushen Chen, <b>Zhikang Niu</b>, Shuai Wang, Kai Yu, Xie Chen.<br />
    <b>Traceable TTS: Toward Watermark-Free TTS with Strong Traceability.</b>
    <br>
    <!-- <a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">11,000+ stars</font></b> on GitHub.<br> -->
    [<a href="https://arxiv.org/abs/2507.03887" target="_blank">Link</a>]
    [<a href="https://arxiv.org/pdf/2507.03887" target="_blank">PDF</a>]
    <!-- [<a href= "https://github.com/SWivid/F5-TTS" target="_blank">Code</a>] -->
    <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:mabuw4cUBgYJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2Pm9TqM8I:AFWwaeYAAAAAZ2zsK8LxbCxd3NGHmZYe6ohDCeA&scisig=AFWwaeYAAAAAZ2zsK5y3MUlvYcPKpHnyK-5x0Oo&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
    <!-- [<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>] -->
    </p>
    </ul>
  </font>

<a name="bench"><h3>Benchmark</h3></a>

<font size="3"> 
<ul>
<li>MMAR Team.<br />
<b>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix.</b><br />
<i><b>NeurIPS Dataset & Benchmark 2025</b></i>,
[<a href="https://arxiv.org/abs/2505.13032" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2505.13032" target="_blank">PDF</a>]
[<a href="https://github.com/ddlBoJack/MMAR" target="_blank">Code</a>]
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:hLAen0w5rmUJ:scholar.google.com/&output=citation&scisdr=ClEsUoQ4EI2Pm309DEs:AFWwaeYAAAAAZ8U7FEtQ0dLvIpQUsNd-L5v3mPY&scisig=AFWwaeYAAAAAZ8U7FLbJloYytyFZSAqT0Xr7j8Q&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li>Ruiqi Yan, Xiquan Li, Wenxi Chen, <b>Zhikang Niu</b>, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen.<br />
<b>URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models.</b><br />
<i><b>EMNLP 2025 Findings</b></i>,
<!-- <i>IEEE International Conference on Acoustics, Speech, and Signal Processing(<b>ICASSP 2025</b>)</i>,  -->
[<a href="https://arxiv.org/abs/2502.17810" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2502.17810" target="_blank">PDF</a>]
[<a href="https://github.com/Ruiqi-Yan/URO-Bench" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:hLAen0w5rmUJ:scholar.google.com/&output=citation&scisdr=ClEsUoQ4EI2Pm309DEs:AFWwaeYAAAAAZ8U7FEtQ0dLvIpQUsNd-L5v3mPY&scisig=AFWwaeYAAAAAZ8U7FLbJloYytyFZSAqT0Xr7j8Q&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li>Zihan Liu*, <b>Zhikang Niu*</b>, Qiuyang Xiao, Zhisheng Zheng, et al.<br />
<b>STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence.</b><br />
<!-- <i><b>EMNLP 2025 Findings</b></i>, -->
<!-- <i>IEEE International Conference on Acoustics, Speech, and Signal Processing(<b>ICASSP 2025</b>)</i>,  -->
[<a href="https://arxiv.org/abs/2510.24693" target="_blank">Link</a>]
[<a href="https://arxiv.org/pdf/2510.24693" target="_blank">PDF</a>]
[<a href="https://github.com/InternLM/StarBench" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:aJTfKlnORLoJ:scholar.google.com/&output=citation&scisdr=ChXMweiAEOuD0KTLDfw:ABGrvjIAAAAAaR_NFfyjJXOgajCbsOOFcUQYWTY&scisig=ABGrvjIAAAAAaR_NFZPlthVcpaLYDIZHcRFY45c&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<a name="Research Projects"><h2>Research Projects</h2></a>
<b>Open-Source Projects</b>:
<font size="3"> 
<ul>

<li><a href="https://github.com/datawhalechina/thorough-pytorch" target="_blank"><b>thorough-pytorch</b>:</a> A Chinese PyTorch tutorial and it has already collected <b><font color="#FF0000">2,300 more stars and 333 forks</font></b> on GitHub.
</li>

 <li><a href="https://github.com/CS-BAOYAN/CSBasicKnowledge" target="_blank"><b>CSBasicKnowledge</b>:</a> This repo will record some knowledge about computer science, artificial intelligence and EE. It has already collected <b><font color="#FF0000">560 more stars</font></b> on GitHub.
</li>

<li>More open-source contents can be found on my <a href="https://github.com/ZhikangNiu" target="_blank">GitHub</a>.
</li>
</ul>
</font>
 
 
<b>Research Projects</b>
<font size="3"> 
<ul>

<li><a href="https://github.com/ZhikangNiu/encodec-pytorch" target="_blank"><b>encodec-pytorch</b>:</a> An unofficial PyTorch implementation of the <a href="https://arxiv.org/pdf/2210.13438.pdf" target="_blank">High Fidelity Neural Audio Compression</a> and it has already collected <b><font color="#FF0000">163 stars</font></b> on GitHub. [<a href="https://huggingface.co/zkniu/encodec-pytorch/tree/main" target="_blank">checkpoint</a>] 
</li>

</ul>
</font>
<a name="Honors and awards"><h2>Honors and Awards</h2></a>
<font size="3"> 
<ul>
<li>2024, Third Prize, 21/1600, Wenxin Cup Entrepreneurship Competition, Baidu.</li>
<li>2024, Stars of Tomorrow, Microsoft Research Asia.</li>
<li>2022, National Scholarship, Ministry of Education in China.</li>
<li>2021, Meritorious Winner, Interdisciplinary Contest In Modeling.</li>
<li>2021, 2023, The First Prize Scholarship, Xidian University.</li>
</ul>
</font>

<a name="Activities"><h2>Activities</h2></a>
<font size="3"> 
<ul>
<li>ICME Reviewer, 2025, 2026</li>
<li>2023.09-2024.9, <a href="https://github.com/CS-BAOYAN" target="_blank">CS-BAOYAN owner</a> (an open-source CS-BAOYAN organization).</li>
<li>2021.11-Now, <a href="https://github.com/datawhalechina" target="_blank">Datawhale member</a> (an open-source AI organization), helped data science fans get involved in the AI community.</li>
<li>2021.11-Now, <a href="https://github.com/X-LANCE/Xmart" target="_blank">Xmart forum maintainer</a> (an open-source student forum from SJTU X-LANCE Lab), for helping students get involved in the speech AI community.</li>
</ul>
</font>

<!-- <A NAME="Invited talks and lives"><h2>Invited Talks and Lives</h2></A>
<font size="3"> 
<ul>
Will be updated in the future. -->
<!-- <li>2023.10, A Speech Multimodal paper sharing presentation.
    [<a href= "https://www.bilibili.com/video/BV18M4y1s7UN/?spm_id_from=333.337.search-card.all.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video</a>]
    [<a href= "https://mp.weixin.qq.com/s/N3OkPlCQwN_c5sxxOZFy2A" target="_blank">Link</a>]
    [<a href= "./Files/Yang-KDDAITIME-2023.jpg" target="_blank">Picture</a>]
</li> 
</li>  
<li>2022.09, New Students Experience Sharing in the SIGS, Tsinghua University.
</li>    
<li>2022.09, Invited Talk in the <a href= "https://www.worldaic.com.cn/" target="_blank">2022 World Artificial Intelligence Conference</a>. (Dishui Lake AI Developer Innovation Forum)
    [<a href= "https://online2022.worldaic.com.cn/forumdetail?uuid=c412de9a58604519940f06cac98fd1fb&type=video" target="_blank">Video1</a>]
    [<a href= "https://www.bilibili.com/video/BV1xW4y1B72o?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video2</a>]
    [<a href= "https://mp.weixin.qq.com/s/xQwPAt6750uShbCiaAXV0w" target="_blank">Link1</a>]
    [<a href= "https://mp.weixin.qq.com/s/d-xrmlGXz28SJsM-VaJLeg" target="_blank">Link2</a>]
    [<a href= "https://mp.weixin.qq.com/s/2I_tp4K3rkEeAPTT1dXgww" target="_blank">Link3</a>]
    [<a href= "https://mp.weixin.qq.com/s/1br3Xs5-t2UjgH-wqEF04g" target="_blank">Link4</a>]
</li> -->

</ul>
</font>

</br></br></br></br>
<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5yaha4wk64q&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=000000&w=500&t=tt&d=mIxY2j5UCMhyHcb61rSeFnsPsRzjjBuB4sPdfTs1dMk&co=ffffff&ct=000000&cmo=3acc3a&cmn=ff5353"></script>
<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script type="text/javascript">
$(function(){
    $(window).scroll(function(){
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop;
        if( scrollt > 400 ) {  
            $("#back_top").fadeIn(400); 
        } else {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){
        $("html,body").animate({scrollTop:"0px"}, 200);
    }); 
});
</script>

<script type="text/javascript">
(function(){
    function onReady(fn){
        if(document.readyState !== 'loading') { fn(); }
        else { document.addEventListener('DOMContentLoaded', fn); }
    }

    var ttlMs = 12 * 60 * 60 * 1000; // 12 hours

    function cacheKey(owner, repo){
        return 'gh_repo_' + owner + '_' + repo;
    }

    function getCached(owner, repo){
        try {
            var raw = localStorage.getItem(cacheKey(owner, repo));
            if(!raw) return null;
            var data = JSON.parse(raw);
            if(!data || !data.ts || (Date.now() - data.ts > ttlMs)) return null;
            return data.value;
        } catch(e) { return null; }
    }

    function setCached(owner, repo, value){
        try {
            localStorage.setItem(cacheKey(owner, repo), JSON.stringify({ ts: Date.now(), value: value }));
        } catch(e) {}
    }

    function parseRepo(href){
        try {
            var url = new URL(href, window.location.href);
            if(url.hostname !== 'github.com') return null;
            var parts = url.pathname.replace(/^\/+/, '').split('/');
            if(parts.length < 2) return null;
            var owner = parts[0];
            var repo = parts[1];
            if(!owner || !repo) return null;
            return { owner: owner, repo: repo };
        } catch(e) { return null; }
    }

    function formatNumber(n){
        if(typeof n !== 'number') return '' + n;
        if(n >= 1000){
            var rounded = Math.round((n / 1000) * 10) / 10;
            return (rounded % 1 === 0 ? rounded.toFixed(0) : rounded.toFixed(1)) + 'k';
        }
        return String(n);
    }

    function createBadge(text){
        var span = document.createElement('span');
        span.className = 'gh-stars-badge';
        span.style.marginLeft = '6px';
        span.style.color = '#555';
        span.style.fontSize = '90%';
        span.textContent = text;
        return span;
    }

    function attachBadge(anchor, stars){
        if(anchor && !anchor.dataset.ghStarsAppended){
            anchor.dataset.ghStarsAppended = '1';
            var badge = createBadge('‚≠ê ' + formatNumber(stars));
            if(anchor.nextSibling){
                anchor.parentNode.insertBefore(badge, anchor.nextSibling);
            } else {
                anchor.parentNode.appendChild(badge);
            }
        }
    }

    function fetchRepo(owner, repo){
        var url = 'https://api.github.com/repos/' + owner + '/' + repo;
        return fetch(url, { headers: { 'Accept': 'application/vnd.github+json' } })
            .then(function(r){ if(!r.ok) throw new Error('HTTP ' + r.status); return r.json(); })
            .then(function(j){ return j && j.stargazers_count; });
    }

    onReady(function(){
        var anchors = Array.prototype.slice.call(document.querySelectorAll('a[href*="github.com/"]'));
        anchors = anchors.filter(function(a){
            return (a.textContent || '').trim() === 'Code';
        });
        if(!anchors.length) return;

        var unique = {};
        var repoToAnchors = {};
        anchors.forEach(function(a){
            var parsed = parseRepo(a.href);
            if(!parsed) return;
            var key = parsed.owner + '/' + parsed.repo;
            unique[key] = parsed;
            if(!repoToAnchors[key]) repoToAnchors[key] = [];
            repoToAnchors[key].push(a);
        });

        Object.keys(unique).forEach(function(key){
            var parsed = unique[key];
            var cached = getCached(parsed.owner, parsed.repo);
            if(cached != null){
                repoToAnchors[key].forEach(function(a){ attachBadge(a, cached); });
                return;
            }
            fetchRepo(parsed.owner, parsed.repo)
                .then(function(stars){
                    if(typeof stars === 'number'){
                        setCached(parsed.owner, parsed.repo, stars);
                        repoToAnchors[key].forEach(function(a){ attachBadge(a, stars); });
                    }
                })
                .catch(function(){ /* ignore errors to avoid blocking page */ });
        });
    });
})();
</script>



<!-- All Rights Reserved by Yiyuan Yang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>. -->


<font size="2" color="#A0A0A0">
<p style="text-align:center">Updating time: 2025.8.20</p>
</font>

</body>
</html>
