<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zhikang Niu, ç‰›å¿—åº·, Audio Signal Processing, Speech Multimodal Large Language Model, Deep Learning, machine learning, Xidian University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./images/book.png">
<title>Zhikang Niu (ç‰›å¿—åº·)</title>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./"><img src="./images/zhikangniu_cartoon.png" alt="" height="220px" style="box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.3); border-radius: 5px;" /></a>&nbsp;</td>
<td align="left"><h1 style="margin-top: 0; margin-bottom: 10px;"><a href="./">Zhikang Niu (<span style="font-family:Microsoft YaHei">ç‰›å¿—åº·</span>)</a></h1>
<br />
Ph.D. Student. 
<a href="https://x-lance.github.io/" target="_blank">Cross Media (X-)Language Intelligence Lab</a><br />
<a href="https://www.cs.sjtu.edu.cn/" target="_blank">Department of Computer Science and Engineering</a>, <a href="https://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University.</a><br />
Work at Shanghai Innovation Institute.
<br />
<br />
<class="staffshortcut">
 <A HREF="#Interest">Research Interests</A> | 
 <A HREF="#Education and intern">Education and Intern</A> | 
 <A HREF="#Publications">Publications</A> | 
 <A HREF="#Projects">Projects</A> | 
 <A HREF="#Honors and awards">Honors and Awards</A> |
 <A HREF="#Activities">Activities</A> | 
 <!-- <A HREF="#Invited talks and lives">Invited Talks and Lives</A> -->
<br />
<br />
if you have any questions, please feel free to contact me with <a href="mailto:zhikangniu@sjtu.edu.cn">zhikangniu@sjtu.edu.cn</a>
<br />
<!-- æ·»åŠ é‚®ä»¶ -->
<!-- [<a href="mailto:zhikangniu@sjtu.edu.cn">Email</a>] -->
[<a href="https://github.com/ZhikangNiu" target="_blank">GitHub</a>] 
[<a href="https://scholar.google.com/citations?user=mXSpi2kAAAAJ&hl=en" target="_blank">Google Scholar</a>] 
[<a href="./assets/WeChat_QR.jpg" target="_blank">WeChat</a>]
<!-- [<a href="./assets/resume-zh_CN.pdf" target="_blank">Chinese Resume</a>] -->
[<a href="./assets/resume_en.pdf" target="_blank">CV</a>]
</td></tr></table>


 
<A NAME="Interest"><h2>Research Interest</h2></A>
I work in the field of Audio Singal Processing, Audio Codec Model,Multimodal Large Language Model, Machine learning, and Deep learning supervised by <a href="https://chenxie95.github.io/" target="_blank">Prof. Xie Chen</a>, I will try my best in the next five exciting years! ðŸ’ª. Currently, I focus on the following research topics:
<ul>
<li>Audio Tokenizer (Discrete and Continuous)</li>
<li>Speech Synthesis (Text to Speech)</li>
<li>Multimodal Large Language Model</li>
</ul>


 
<A NAME="Education and intern"><h2>Education and Intern</h2></A>
<ul>
<li>2025.01-current &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Intern, <a href="https://www.shlab.org.cn/" target="_blank">Shanghai Artificial Intelligence Laboratory</a>.</li>
<li>2023.08-2024.09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Intern, Natural Language Computing Group (NLC), <a href="https://www.msra.cn/" target="_blank">Microsoft Research Asia (MSRA)</a>. Led by <a href="https://thegenerality.com/",target="_blank">Furu Wei</a>, supervised by <a href="https://www.microsoft.com/en-us/research/people/shujliu/" target="_blank">Shujie Liu</a> and <a href="https://long-zhou.github.io/" target="_blank">Long Zhou</a>. Focus on Audio Codec and Speech Synthesis.</li> 
<li>2020.09-2024.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bachelor Degree of Engineering, <a href="https://sai.xidian.edu.cn/" target="_blank">School of Artificial Intelligence</a>, <a href="https://www.xidian.edu.cn/" target="_blank">Xidian University</a>.</li>
</ul>

 
<A NAME="Publications"><h2>Publications</h2></A>
<A NAME="tts/omni"><h3>Speech Synthesis/Omni System</h3></A>
<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> <b>Zhikang Niu</b>, Sanyuan Chen, Long Zhou, Ziyang Ma, Xie Chen, Shujie Liu.<br>
<b>NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization.</b><br>
<i><b>SLT 2024</b></i>,
[<a href= "https://arxiv.org/abs/2409.12717" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2409.12717.pdf" target="_blank">PDF</a>]
[<a href= "https://github.com/ZhikangNiu/NDVQ" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:iZGY-fGY7V0J:scholar.google.com/&output=citation&scisdr=ClE9Ykj8EOKBzLPFleU:AFWwaeYAAAAAZwfDjeWrAPZjPpJAhx24s6n-28I&scisig=AFWwaeYAAAAAZwfDjRXdVka9BJmZmd1l7f2Pq5s&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Jeongsoo Choi<sup>*</sup>, <b>Zhikang Niu<sup>*</sup></b>, Ji-Hoon Kim, Chunhui Wang, Joon Son Chung, Xie Chen.<br>
<b>Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment.</b><br>
<i><b>InterSpeech 2025</b></i>,
<!-- [<a href= "https://arxiv.org/abs/2409.12717" target="_blank">Link</a>] -->
<!-- [<a href= "https://arxiv.org/pdf/2409.12717.pdf" target="_blank">PDF</a>] -->
<!-- [<a href= "https://github.com/ZhikangNiu/NDVQ" target="_blank">Code</a>] -->
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:iZGY-fGY7V0J:scholar.google.com/&output=citation&scisdr=ClE9Ykj8EOKBzLPFleU:AFWwaeYAAAAAZwfDjeWrAPZjPpJAhx24s6n-28I&scisig=AFWwaeYAAAAAZwfDjRXdVka9BJmZmd1l7f2Pq5s&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li> Yushen Chen, <b>Zhikang Niu</b>, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen.<br>
<b>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.</b><br>
<i><b>ACL 2025 Main</b></i>,
[<a href= "https://arxiv.org/abs/2410.06885" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2410.06885" target="_blank">PDF</a>]
[<a href= "https://github.com/SWivid/F5-TTS" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:mabuw4cUBgYJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2Pm9TqM8I:AFWwaeYAAAAAZ2zsK8LxbCxd3NGHmZYe6ohDCeA&scisig=AFWwaeYAAAAAZ2zsK5y3MUlvYcPKpHnyK-5x0Oo&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
[<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>]<br>
<a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">12,000+ stars</font></b> on GitHub.<br>
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Qixi zheng, Yushen Chen, <b>Zhikang Niu</b>, Ziyang Ma, Xiaofei Wang, Kai Yu, Xie Chen.<br>
<b>Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling.</b><br>
<i><b>InterSpeech 2025</b></i>,
<!-- [<a href= "https://arxiv.org/abs/2409.12717" target="_blank">Link</a>] -->
<!-- [<a href= "https://arxiv.org/pdf/2409.12717.pdf" target="_blank">PDF</a>] -->
<!-- [<a href= "https://github.com/ZhikangNiu/NDVQ" target="_blank">Code</a>] -->
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:iZGY-fGY7V0J:scholar.google.com/&output=citation&scisdr=ClE9Ykj8EOKBzLPFleU:AFWwaeYAAAAAZwfDjeWrAPZjPpJAhx24s6n-28I&scisig=AFWwaeYAAAAAZwfDjRXdVka9BJmZmd1l7f2Pq5s&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li> Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, <b>Zhikang Niu</b>, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen.<br>
<b>Towards Flow-Matching-based TTS without Classifier-Free Guidance.</b>
<!-- <a href="https://github.com/SWivid/F5-TTS" target="_blank">F5-TTS</a> has collected <b><font color="#FF0000">11,000+ stars</font></b> on GitHub.<br> -->
[<a href= "https://arxiv.org/abs/2504.20334" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2504.20334" target="_blank">PDF</a>]
<!-- [<a href= "https://github.com/SWivid/F5-TTS" target="_blank">Code</a>] -->
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:mabuw4cUBgYJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2Pm9TqM8I:AFWwaeYAAAAAZ2zsK8LxbCxd3NGHmZYe6ohDCeA&scisig=AFWwaeYAAAAAZ2zsK5y3MUlvYcPKpHnyK-5x0Oo&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
<!-- [<a href="https://event.baai.ac.cn/live/852" target="_blank">Talk</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li>Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, <b>Zhikang Niu</b>, et al.<br>
<b>SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training.</b><br>
<i><b>ACL 2025 Findings</b></i>,
[<a href= "https://arxiv.org/abs/2412.15649" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2412.15649" target="_blank">PDF</a>]
[<a href= "https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/s2s" target="_blank">Code</a>]
[<a href="http://scholar.googleusercontent.com/scholar.bib?q=info:0ba6dZka5owJ:scholar.google.com/&output=citation&scisdr=ClEsUoQnEI2PlKmz4X0:AFWwaeYAAAAAaBG1-X04qYsd1dBVPEvMkKcpPb8&scisig=AFWwaeYAAAAAaBG1-R91WNJpVLpV4tYnSKLeoeA&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, <b>Zhikang Niu</b>, Shuai Wang, Hui Zhang, Xie Chen.<br>
<b>VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech.</b><br>
<i><b>ICASSP 2025</b></i>,
[<a href= "https://arxiv.org/abs/2401.14321" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2401.14321.pdf" target="_blank">PDF</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:423LVFzDAh0J:scholar.google.com/&output=citation&scisdr=ClEXtQ9QEMjm1kg_lBQ:AFWwaeYAAAAAZeE5jBRZItSkmZL-6cy9PIFICTI&scisig=AFWwaeYAAAAAZeE5jKmKyS0hsW4Yk5zbPIk00-M&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Guanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, <b>Zhikang Niu</b>, Xie Chen.<br>
<b>Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.</b><br>
<i><b>ASRU 2023</b></i>,
[<a href= "https://arxiv.org/abs/2309.13860" target="_blank">Link</a>]
[<a href= "https://browse.arxiv.org/pdf/2309.13860.pdf" target="_blank">PDF</a>]
[<a href= "https://github.com/yanghaha0908/FastHuBERT" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5igU4Z9Xe2sJ:scholar.google.com/&output=citation&scisdr=ClEoLDBxEOzd6Ydut8w:AFWwaeYAAAAAZSFor8xL1SdSCeYwkCTVnwkGQSo&scisig=AFWwaeYAAAAAZSFor1SCJ3LMXRhW0icv0dF0A4c&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<A NAME="bench"><h3>Benchmark</h3></A>

<font size="3"> 
<ul>
<li>MMAR Team.<br>
<b>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix.</b><br>
<!-- <i>IEEE International Conference on Acoustics, Speech, and Signal Processing(<b>ICASSP 2025</b>)</i>,  -->
[<a href= "https://arxiv.org/abs/2505.13032" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2505.13032" target="_blank">PDF</a>]
[<a href= "https://github.com/ddlBoJack/MMAR" target="_blank">Code</a>]
<!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:hLAen0w5rmUJ:scholar.google.com/&output=citation&scisdr=ClEsUoQ4EI2Pm309DEs:AFWwaeYAAAAAZ8U7FEtQ0dLvIpQUsNd-L5v3mPY&scisig=AFWwaeYAAAAAZ8U7FLbJloYytyFZSAqT0Xr7j8Q&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>] -->
</p>
</ul>
</font>

<font size="3"> 
<ul>
<li>Ruiqi Yan, Xiquan Li, Wenxi Chen, <b>Zhikang Niu</b>, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen.<br>
<b>URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models.</b><br>
<!-- <i>IEEE International Conference on Acoustics, Speech, and Signal Processing(<b>ICASSP 2025</b>)</i>,  -->
[<a href= "https://arxiv.org/abs/2502.17810" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2502.17810" target="_blank">PDF</a>]
[<a href= "https://github.com/Ruiqi-Yan/URO-Bench" target="_blank">Code</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:hLAen0w5rmUJ:scholar.google.com/&output=citation&scisdr=ClEsUoQ4EI2Pm309DEs:AFWwaeYAAAAAZ8U7FEtQ0dLvIpQUsNd-L5v3mPY&scisig=AFWwaeYAAAAAZ8U7FLbJloYytyFZSAqT0Xr7j8Q&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

<A NAME="Projects"><h2>Projects</h2></A>
<b>Open-Source Projects</b>:
<font size="3"> 
<ul>

<li><a href= "https://github.com/datawhalechina/thorough-pytorch" target="_blank"><b>thorough-pytorch</b>:</a> A Chinese PyTorch tutorial and it has already collected <b><font color="#FF0000">2,300 more stars and 333 forks</font></b> on GitHub.
</li>

 <li><a href= "https://github.com/CS-BAOYAN/CSBasicKnowledge" target="_blank"><b>CSBasicKnowledge</b>:</a> This repo will record some knowledge about computer science, artificial intelligence and EE. It has already collected <b><font color="#FF0000">560 more stars</font></b> on GitHub.
</li>

<li>More open-source contents can be found on my <a href= "https://github.com/ZhikangNiu" target="_blank">GitHub</a>.
</li>
</ul>
</font>
 
 
<b>Research Projects</b>
<font size="3"> 
<ul>

<li><a href= "https://github.com/ZhikangNiu/encodec-pytorch" target="_blank"><b>encodec-pytorch</b>:</a> An unofficial PyTorch implementation of the <a href= "https://arxiv.org/pdf/2210.13438.pdf" target="_blank">High Fidelity Neural Audio Compression</a> and it has already collected <b><font color="#FF0000">155 stars</font></b> on GitHub. [<a href= "https://huggingface.co/zkniu/encodec-pytorch/tree/main" target="_blank">checkpoint</a>] 
</li>

</ul>
</font>
<A NAME="Honors and awards"><h2>Honors and Awards</h2></A>
<font size="3"> 
<ul>
<li>2024, Stars of Tomorrow, Microsoft Research Asia.</li>
<li>2022, National Scholarship, Ministry of Education in China.</li>
<li>2021, Meritorious Winner, Interdisciplinary Contest In Modeling.</li>
<li>2021, 2023, The First Prize Scholarship, Xidian University.</li>
</ul>
</font>

<A NAME="Activities"><h2>Activities</h2></A>
<font size="3"> 
<ul>
<li>2023.09-2024.9, <a href= "https://github.com/CS-BAOYAN" target="_blank">CS-BAOYAN owner</a> (an open-source CS-BAOYAN organization).</li>
<li>2021.11-Now, <a href= "https://datawhale.club/" target="_blank">Datawhale member</a> (an open-source AI organization), helped data science fans get involved in the AI community.</li>
</ul>
</font>

<!-- <A NAME="Invited talks and lives"><h2>Invited Talks and Lives</h2></A>
<font size="3"> 
<ul>
Will be updated in the future. -->
<!-- <li>2023.10, A Speech Multimodal paper sharing presentation.
    [<a href= "https://www.bilibili.com/video/BV18M4y1s7UN/?spm_id_from=333.337.search-card.all.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video</a>]
    [<a href= "https://mp.weixin.qq.com/s/N3OkPlCQwN_c5sxxOZFy2A" target="_blank">Link</a>]
    [<a href= "./Files/Yang-KDDAITIME-2023.jpg" target="_blank">Picture</a>]
</li> 
</li>  
<li>2022.09, New Students Experience Sharing in the SIGS, Tsinghua University.
</li>    
<li>2022.09, Invited Talk in the <a href= "https://www.worldaic.com.cn/" target="_blank">2022 World Artificial Intelligence Conference</a>. (Dishui Lake AI Developer Innovation Forum)
    [<a href= "https://online2022.worldaic.com.cn/forumdetail?uuid=c412de9a58604519940f06cac98fd1fb&type=video" target="_blank">Video1</a>]
    [<a href= "https://www.bilibili.com/video/BV1xW4y1B72o?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video2</a>]
    [<a href= "https://mp.weixin.qq.com/s/xQwPAt6750uShbCiaAXV0w" target="_blank">Link1</a>]
    [<a href= "https://mp.weixin.qq.com/s/d-xrmlGXz28SJsM-VaJLeg" target="_blank">Link2</a>]
    [<a href= "https://mp.weixin.qq.com/s/2I_tp4K3rkEeAPTT1dXgww" target="_blank">Link3</a>]
    [<a href= "https://mp.weixin.qq.com/s/1br3Xs5-t2UjgH-wqEF04g" target="_blank">Link4</a>]
</li> -->

</ul>
</font>

</br></br></br></br>
<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5yaha4wk64q&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=000000&w=500&t=tt&d=mIxY2j5UCMhyHcb61rSeFnsPsRzjjBuB4sPdfTs1dMk&co=ffffff&ct=000000&cmo=3acc3a&cmn=ff5353'></script>
<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>



<!-- All Rights Reserved by Yiyuan Yang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>. -->


<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2025.5.16</p>
</font>

</body>
</html>
